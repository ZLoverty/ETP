"""
Utility functions for data analysis and processing.
"""

import numpy as np
import pandas as pd
from pathlib import Path
from scipy.optimize import curve_fit
from scipy.interpolate import CubicSpline

def linear(x, a):
    return a * x

def linear_offset(x, a, b):
    return a * x + b

def find_force_transition_velocity(df, velocity_col_name="速度", force_col_name="挤出力", thres=0.01):
    """Fit linear function to the extrusion force data and find the transition from slow increase regime to fast increase regime by identifying the first bad linear fit. 
    
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the velocity and force data to be analyzed.
    velocity_col_name : str, optional
        Name of the column containing velocity data, by default "速度"
    force_col_name : str, optional
        Name of the column containing force data, by default "挤出力"
    thres : float, optional
        Threshold for the mean squared error to identify bad linear fit, by default 0.01
    
    Returns
    -------
    float
        The velocity at which the transition occurs, or None if not found.

    Edit
    ----
    * Sep 23, 2025: drop nan values in input DataFrame.
    """

    df = df.dropna()
    nPoints_list = range(3, len(df))
    vmax_list = []
    mean_err_list = []
    for nPoints in nPoints_list:
        x = df[velocity_col_name][:nPoints]
        y = df[force_col_name][:nPoints]
        popt, pcov = curve_fit(linear_offset, x, y)
        mean_err = ((y - linear_offset(x, *popt)) ** 2).sum() / nPoints / np.mean(y)**2
        # plt.plot(x, linear_offset(x, *popt), lw=1, ls="--")
        vmax_list.append(x.max())
        mean_err_list.append(mean_err)

    x = np.array(vmax_list)
    y = np.array(mean_err_list)
    if len(x) > 2:
        cs = CubicSpline(x, y, bc_type="natural")
        x_fine = np.linspace(x.min(), x.max(), 100)
        y_fine = cs(x_fine)
        transition_points = x_fine[y_fine > thres]
        if len(transition_points) > 0:
            return transition_points[0]
        else:
            return None
    else:
        raise ValueError("Not enough data points to perform cubic spline interpolation.")
    
def find_steady_state_start(data_series, window_size, threshold):
    """
    Finds the index where a time series reaches a steady state.

    Parameters
    ----------
    data_series : pd.Series
        The time series data.
    window_size : int 
        The number of data points in the sliding window.
    threshold : float 
        The standard deviation threshold to define a steady state.

    Returns
    -------
    int 
        The index of the start of the steady state, or None if not found.
    """
    # reset index, ignore the original index
    data_series = data_series.reset_index(drop=True)
    
    # Calculate the rolling standard deviation
    rolling_std = data_series.rolling(window=window_size).std() / data_series.rolling(window=window_size).mean()

    # Find the first index where the rolling_std is below the threshold
    steady_state_candidates = rolling_std[rolling_std < threshold]

    if not steady_state_candidates.empty:
        # The start of the steady-state REGIME is the first point in that first window
        # So we get the index of the first window that met the criteria
        first_window_end_index = steady_state_candidates.index[0]
        # The actual start of the data regime is the beginning of that window
        steady_state_start_index = max(0, first_window_end_index - window_size + 1)
        return steady_state_start_index
    else:
        return None # No steady state found
    

def clean_data(raw_data: pd.DataFrame,
               step_length: int = 600,
               velocity_step: float = 0.8,
               feedrate_min: float = 0.2,
               force_min: float = 0.5,
               feedrate_col: str = "feedrate_mms"):
    """
    Parameters
    ----------
    raw_data : pd.DataFrame
        the original table generated by HEPiC
    step_length : int
        the number of rows of data at each setting. This is used to infer the smoothing window size. 
    velocity_step : float
        the interval between adjacent velocity step (in mm/s). This is used to infer the threshold for the velocity gradient. If gradient is greater than the threshold, a new step is recognized.
    feedrate_min : float
        minimum feedrate. This is used to filter the data entries where no extrusion is happening.
    force_min : float
        minimum extrusion force. This is used to filter the data entries where the extrusion force is extremely small. 

    Returns
    -------
    cleaned_steps : list
        a list of pd.DataFrame, each of which is regarded as a single parameter set. 
    data_clean : pd.DataFrame
        the cleaned data table.

    Examples
    --------
    >>> cleaned_steps, data_clean = clean_data(data)
    """

    # remove tiny force entries
    force_min = 0.5 # N
    data_clean = raw_data.loc[(raw_data.extrusion_force_N >= force_min)&(raw_data[feedrate_col]>feedrate_min)]

    # time start at 0
    data_clean.loc[:, "time_s"] -= data_clean.time_s.min()

    window = step_length // 10
    min_length = step_length // 2
    threshold = velocity_step / 10
    diff_length = step_length // 10

    trim_length = step_length // 10

    ####
    data_clean.loc[:, "smooth_feedrate"] = data_clean.loc[:, feedrate_col].rolling(window).mean().values
    data_clean.loc[:, "gradient"] = data_clean["smooth_feedrate"].diff(diff_length).abs()
    data_clean.loc[:, "is_flat"] = data_clean["gradient"].lt(threshold)
    data_clean.loc[:, "step_id"] = (~data_clean["is_flat"]).cumsum()

    cleaned_steps = []
    for step, g_step in data_clean.groupby("step_id"):
        if len(g_step) > min_length:
            cleaned_steps.append(g_step[:-trim_length])
    
    print(f"number of steps: {len(cleaned_steps)}")

    return cleaned_steps, data_clean